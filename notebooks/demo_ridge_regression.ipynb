{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Ridge Regression on the Diabetes Dataset\n\nThis notebook walks through a simple Ridge Regression workflow using scikit-learn's built-in diabetes dataset.\nIt highlights why L2 regularization is useful and how changing the `alpha` parameter affects model behavior."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Why Ridge Regression?\n\n* **Regularization**: Adds an L2 penalty to discourage large coefficients and reduce overfitting.\n* **Numerical stability**: Helps when features are correlated or when the dataset is small/noisy.\n* **`alpha` controls strength**: Larger values increase shrinkage; smaller values behave closer to ordinary least squares.\n* **Scaling matters**: Because the penalty depends on coefficient magnitudes, features should be on comparable scales (hence `StandardScaler`)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import sys\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_diabetes\n\n# Allow importing from the app/ directory\nrepo_root = Path.cwd().resolve().parent\nif str(repo_root) not in sys.path:\n    sys.path.append(str(repo_root))\n\nfrom app.data import load_diabetes_dataset\nfrom app.evaluate import regression_metrics\nfrom app.model import build_ridge_model"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Load and inspect the data"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "X, y = load_diabetes_dataset()\nX.head(), y.head()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Train/test split\nWe reserve 20% of the data for testing."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.shape, X_test.shape"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Train Ridge models with different `alpha` values\nWe use a Pipeline to ensure scaling happens inside the training workflow."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "alphas = [0.1, 1.0, 10.0]\nresults = {}\n\nfor alpha in alphas:\n    model = build_ridge_model(alpha=alpha)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    results[alpha] = regression_metrics(y_test, pd.Series(preds, index=y_test.index))\n\nresults"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualize predicted vs actual for `alpha=1.0`\nThe closer points lie to the diagonal line, the better the predictions."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "alpha = 1.0\nmodel = build_ridge_model(alpha=alpha)\nmodel.fit(X_train, y_train)\npreds = pd.Series(model.predict(X_test), index=y_test.index)\n\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x=y_test, y=preds, alpha=0.7, edgecolor=\"white\")\nline_min, line_max = min(y_test.min(), preds.min()), max(y_test.max(), preds.max())\nplt.plot([line_min, line_max], [line_min, line_max], linestyle=\"--\", color=\"red\")\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.title(\"Ridge Regression (alpha=1.0)\")\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Next steps\n\n* Try more `alpha` values and plot how metrics change.\n* Use cross-validation (`RidgeCV`) to pick an optimal `alpha`.\n* Add feature importance analysis or coefficient inspection."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}